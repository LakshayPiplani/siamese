{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMpu-IsI6vUx"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "4F-0BWLbmLyF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "kop8w6aGBLQl"
      },
      "outputs": [],
      "source": [
        "## need to get Sciq data in the right format. waiting on Balaji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "Cyddnde5BLgR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cK0vVj-ZKSC0",
        "outputId": "d17d0f24-ba1d-4f65-c3ee-590a4125a9c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prJmTm9cmLyK"
      },
      "source": [
        "train_data = pd.read_csv(\"sample_data/train_mod.csv\",\n",
        "                         quotechar = '\"'\n",
        "                        #  quoting=csv.QUOTE_ALL,\n",
        "# on_bad_lines='skip'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "RUcHBcA06vU1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "6cd4a8bf-2358-4e9a-dede-9d59db62bd70"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-b8ead992bdda>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample_data/train.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1321\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             )\n\u001b[1;32m   1323\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected object or value"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_json(\"sample_data/train.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAVXFb9I6vU2"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_json(\"sample_data/test.json\")"
      ],
      "metadata": {
        "id": "Grf4I16a9y9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDbvJy2o6vU2"
      },
      "outputs": [],
      "source": [
        "# df_1 = df[['support', 'correct_answer']]\n",
        "# df_1['target'] = 'Similar'\n",
        "\n",
        "# df_2 = df[['support', 'distractor1']]\n",
        "# df_2['target'] = 'Dissimilar'\n",
        "\n",
        "# pd.concat([df_1, df_2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipQHrca76vU3"
      },
      "outputs": [],
      "source": [
        "df_ca = train_data.loc[:,['support','correct_answer']]\n",
        "df_ca['target'] = 'Similar'\n",
        "df_da1 = train_data.loc[:,['support','distractor1']].rename(columns = {'distractor1':'correct_answer'})\n",
        "df_da1['target'] = 'Dissimilar'\n",
        "pd.concat([df_ca,df_da1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF2Czez96vU3"
      },
      "outputs": [],
      "source": [
        "df_ca = train_data.loc[:,['support','correct_answer']].rename(columns={'correct_answer':'response'})\n",
        "df_ca['target'] = 'Similar'\n",
        "df_da1 = train_data.loc[:,['support','distractor1']].rename(columns = {'distractor1':'response'})\n",
        "df_da1['target'] = 'Dissimilar'\n",
        "df_da2 =train_data.loc[:,['support','distractor2']].rename(columns = {'distractor2':'response'})\n",
        "df_da2['target'] = 'Dissimilar'\n",
        "df_da3 =train_data.loc[:,['support','distractor3']].rename(columns = {'distractor3':'response'})\n",
        "df_da3['target'] = 'Dissimilar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4u3hAdu6vU4"
      },
      "outputs": [],
      "source": [
        "df_result = pd.concat([df_ca,df_da1,df_da2,df_da3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.shape"
      ],
      "metadata": {
        "id": "8-al086X5fT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc7853Di6vU5"
      },
      "outputs": [],
      "source": [
        "df_result.iloc[0]['support']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCHdTOMv6vU5"
      },
      "outputs": [],
      "source": [
        "df_result.loc[df_result['support'] == \"Mesophiles grow best in moderate temperature, typically between 25°C and 40°C (77°F and 104°F). Mesophiles are often found living in or on the bodies of humans or other animals. The optimal growth temperature of many pathogenic mesophiles is 37°C (98°F), the normal human body temperature. Mesophilic organisms have important uses in food preparation, including cheese, yogurt, beer and wine.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDqJ7ggb6vU5"
      },
      "outputs": [],
      "source": [
        "df_result.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3T-oSuqPni_"
      },
      "outputs": [],
      "source": [
        "train_data.head(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0Rj45H16vU6"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpUhLfKC6vU8"
      },
      "outputs": [],
      "source": [
        "# print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twPI3TlG6vU8"
      },
      "outputs": [],
      "source": [
        "include_words = set([''])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD9bS5aGmLzJ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def striphtml(data):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', str(data))\n",
        "    return cleantext\n",
        "\n",
        "def stripunc(data):\n",
        "#     return re.sub('[^A-Za-z]+', ' ', str(data), flags=re.MULTILINE|re.DOTALL)\n",
        "    return re.sub('[.,;!?\\\\-\\'()]', ' ', str(data), flags=re.MULTILINE|re.DOTALL)\n",
        "\n",
        "\n",
        "def compute(sent):\n",
        "\n",
        "    sent = decontracted(sent)\n",
        "#     sent = striphtml(sent)\n",
        "    sent = stripunc(sent)\n",
        "\n",
        "    words=word_tokenize(str(sent.lower()))\n",
        "    include_words = set()\n",
        "\n",
        "    #Removing all single letter and and stopwords from question\n",
        "#     sent1=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1))\n",
        "#     sent2=' '.join(str(j) for j in words if j not in stop_words and (len(j)!=1))\n",
        "    sent1 =' '.join(str(stemmer.stem(j))  for j in words )\n",
        "    sent2 =' '.join(str(j)  for j in words )\n",
        "    return sent1, sent2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw6iWKgT6vU8"
      },
      "outputs": [],
      "source": [
        "df_temp_support = pd.DataFrame(df_result.loc[:,'support'].apply(compute).to_list(),index = df_result.index)\n",
        "\n",
        "df_result['support_stemmed'] = df_temp_support[0]\n",
        "df_result['support_unstemmed'] = df_temp_support[1]\n",
        "\n",
        "df_temp_response = pd.DataFrame(df_result.loc[:,'response'].apply(compute).to_list(),index = df_result.index)\n",
        "df_result['response_stemmed'] = df_temp_response[0]\n",
        "df_result['response_unstemmed'] = df_temp_response[1]\n",
        "\n",
        "df_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUOYO2mm6vU8"
      },
      "outputs": [],
      "source": [
        "combined_unstemmed_text = list(df_result['support_unstemmed'].str.cat(' '+df_result['response_unstemmed']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(combined_unstemmed_text)"
      ],
      "metadata": {
        "id": "E0Sqnrrj5EPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_org_text = list(df_result['support'].str.cat(' '+df_result['response']))"
      ],
      "metadata": {
        "id": "oF650w5Y575I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(combined_org_text)"
      ],
      "metadata": {
        "id": "M7xHQcn051I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7dL3mLL6vU9"
      },
      "outputs": [],
      "source": [
        "# combined_org_text = list(df_result['support'].str.cat(';;'+df_result['response']).iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.shape"
      ],
      "metadata": {
        "id": "OJj0cz_55mc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urTBYZoAmL0h"
      },
      "outputs": [],
      "source": [
        "df_result['combined_unstemmed_text'] = combined_unstemmed_text\n",
        "df_result['combined_org_text'] = combined_org_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG4fx8oLmL0o"
      },
      "outputs": [],
      "source": [
        "# test_data['clean_stemmed_q1_t'] = clean_stemmed_q1_t\n",
        "# test_data['clean_stemmed_q2_t'] = clean_stemmed_q2_t\n",
        "# test_data['clean_q1_t'] = clean_q1_t\n",
        "# test_data['clean_q2_t'] = clean_q2_t\n",
        "# test_data['combined_stemmed_text_t'] = combined_stemmed_text_t\n",
        "\n",
        "test_data = pd.read_json(\"sample_data/test.json\")\n",
        "\n",
        "test_df_ca = test_data.loc[:,['support','correct_answer']].rename(columns={'correct_answer':'response'})\n",
        "test_df_ca['target'] = 'Similar'\n",
        "test_df_da1 = test_data.loc[:,['support','distractor1']].rename(columns = {'distractor1':'response'})\n",
        "test_df_da1['target'] = 'Dissimilar'\n",
        "test_df_da2 =test_data.loc[:,['support','distractor2']].rename(columns = {'distractor2':'response'})\n",
        "test_df_da2['target'] = 'Dissimilar'\n",
        "test_df_da3 =test_data.loc[:,['support','distractor3']].rename(columns = {'distractor3':'response'})\n",
        "test_df_da3['target'] = 'Dissimilar'\n",
        "\n",
        "test_data = pd.concat([test_df_ca,test_df_da1,test_df_da2,test_df_da3])\n",
        "\n",
        "test_data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_support = pd.DataFrame(test_data.loc[:,'support'].apply(compute).to_list(),index = test_data.index)\n",
        "\n",
        "test_data['support_stemmed'] = df_test_support[0]\n",
        "test_data['support_unstemmed'] = df_test_support[1]\n",
        "\n",
        "df_test_response = pd.DataFrame(test_data.loc[:,'response'].apply(compute).to_list(),index = test_data.index)\n",
        "test_data['response_stemmed'] = df_test_response[0]\n",
        "test_data['response_unstemmed'] = df_test_response[1]"
      ],
      "metadata": {
        "id": "Ycq7QQsqELrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "e026xiQmJ4EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek6rQUHbmL0s"
      },
      "outputs": [],
      "source": [
        "train_data = df_result.copy()\n",
        "train_data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAz2aD8CmL4O"
      },
      "outputs": [],
      "source": [
        "# test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FLs1aW9mL46"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w1hsvBdmL4-"
      },
      "outputs": [],
      "source": [
        "# X_temp, X_test, y_temp, y_test = train_test_split(data[['clean_q1', 'clean_q2']], data['is_duplicate'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONHFFyRRmL5B"
      },
      "outputs": [],
      "source": [
        "# X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF7f-anacvEs"
      },
      "outputs": [],
      "source": [
        "# X_train = train_data[['support', 'response']]\n",
        "# y_train = train_data['target']\n",
        "\n",
        "# X_test = test_data[['clean_q1_t', 'clean_q2_t']]\n",
        "# y_test = test_data['is_duplicate']\n",
        "\n",
        "X_train = df_result[['support_unstemmed', 'response_unstemmed']]\n",
        "y_train = df_result['target']\n",
        "\n",
        "X_test = test_data[['support_unstemmed', 'response_unstemmed']]\n",
        "y_test = test_data['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9plmc42mL5E"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "# print(X_val.shape)\n",
        "# print(y_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vZer1RImL5H"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.head()"
      ],
      "metadata": {
        "id": "_V3SBi6W67h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r34zQG4XmL5M"
      },
      "outputs": [],
      "source": [
        "X_train['text'] = X_train[['support_unstemmed','response_unstemmed']].apply(lambda x:str(x[0])+\" \"+str(x[1]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCnEvQszmL5O"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B5BMr1XmL5R"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e989XTAmL5b"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6ZeCAB2mL5d"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Input, Concatenate, Conv2D, Flatten, Dense, Embedding, LSTM\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsLlWhg0mL6G"
      },
      "outputs": [],
      "source": [
        "! pip install joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxwRI9cimL6J"
      },
      "outputs": [],
      "source": [
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsvcmaWuH1PK"
      },
      "source": [
        "Going a bit off-track here from the medium article.\n",
        "\n",
        "Instead, following a more up-to-date embedding approach as mentioned [here](https://keras.io/examples/nlp/pretrained_word_embeddings/).\n",
        "\n",
        "***START OF NEW EMBEDDING APRROACH***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6bCjGIEH0d9"
      },
      "outputs": [],
      "source": [
        "# TextVectorization documentation here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=10)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices((X_train['support_unstemmed'] + ' ' +  X_train['response_unstemmed']).values).batch(256)\n",
        "vectorizer.adapt(text_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5pOvEtCH0kH"
      },
      "outputs": [],
      "source": [
        "vectorizer.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbvcDwKmK14d"
      },
      "outputs": [],
      "source": [
        "output = vectorizer([[\"share market history are airpl\"]])\n",
        "output.numpy()\n",
        "\n",
        "# value 1 in the output represents out-of-voccabulary words\n",
        "# the other integer values just represent the index position of the word in the vocabulary\n",
        "# for eg capricorn is represented as 3 because it is at 4th place in the vocab list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(output)"
      ],
      "metadata": {
        "id": "8yL18Gkw7Qp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXLrITYPOt5G"
      },
      "outputs": [],
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mlJGo2yOvxO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le73R_-jK17Z"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLK6gZAyK1-N"
      },
      "outputs": [],
      "source": [
        "path_to_glove_file = 'glove.6B.100d.txt'\n",
        "\n",
        "cnt = 0\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        if cnt < 5:\n",
        "          print(line, '\\n')\n",
        "          cnt += 1\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        #maxsplit = 1 means split on the first white space, which separates actual word on the left from the vector on the right\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjwo0RRVOXi6"
      },
      "outputs": [],
      "source": [
        "type(embeddings_index['hello'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMop1TQHOl8w"
      },
      "source": [
        "Now, let's prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALEEUsu3OfgQ"
      },
      "outputs": [],
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "missed_words=[]\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        missed_words.append(word)\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missed_words[:5]"
      ],
      "metadata": {
        "id": "XRGaOsSKKtFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Why are we doing this???\n",
        "\n",
        "\n",
        "# #Trying feeding text directly into the embedding matrix without token vectorizer\n",
        "# num_tokens = len(voc) + 2\n",
        "# embedding_dim = 100\n",
        "# hits = 0\n",
        "# misses = 0\n",
        "\n",
        "# embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "# for word in X_train['text']:\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # Words not found in embedding index will be all-zeros.\n",
        "#         # This includes the representation for \"padding\" and \"OOV\"\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#         hits += 1\n",
        "#     else:\n",
        "#         misses += 1\n",
        "# print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "id": "3X7Iync1GZGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp9kJJ6lQMiH"
      },
      "source": [
        "Next, we load the pre-trained word embeddings matrix into an Embedding layer.\n",
        "\n",
        "Note that we set trainable=False so as to keep the embeddings fixed (we don't want to update them during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4l6RxDXOfjQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78zwHILIISg"
      },
      "source": [
        "***END OF NEW EMBEDDING APRROACH***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr6ipO3tQiGo"
      },
      "source": [
        "Now we start building Siamese using embeddings generated from new approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDnsm5KTv2G8"
      },
      "outputs": [],
      "source": [
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rk8FTaOjyQjc"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def cosine_distance(vests):\n",
        "  x, y = vests\n",
        "  x = K.l2_normalize(x, axis=-1)\n",
        "  y = K.l2_normalize(y, axis=-1)\n",
        "  return -K.mean(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "def cos_dist_output_shape(shapes):\n",
        "  shape1, shape2 = shapes\n",
        "  return (shape1[0],1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd6bzWhMy1z5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def auroc(y_true, y_pred):\n",
        "  return roc_auc_score(y_true, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXc-22AIXlqq"
      },
      "source": [
        "Now we need\n",
        "\n",
        "1. String data of q1\n",
        "2. Convert it into index-vector\n",
        "3. Convert index-vector into embedding space"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "J15MGDmtNEqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([[s] for s in X_train['support_unstemmed']])\n",
        "arr.shape"
      ],
      "metadata": {
        "id": "_0sPSByf97Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ec3rEnRXPBC"
      },
      "outputs": [],
      "source": [
        "index_vec_1 = vectorizer([s for s in X_train['support_unstemmed']])\n",
        "index_vec_2 = vectorizer([s for s in X_train['response_unstemmed']])\n",
        "\n",
        "\n",
        "# y_train is already defined\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "KsAajE4FNpRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.loc[y_train == 'Similar'] = 1\n",
        "y_train.loc[y_train == 'Dissimilar'] = 0\n",
        "y_train[:10]"
      ],
      "metadata": {
        "id": "TRork5dnNRMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "9dgmhdOXNYQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.loc[y_test == 'Similar'] = 1\n",
        "y_test.loc[y_test == 'Dissimilar'] = 0\n",
        "y_test.value_counts()"
      ],
      "metadata": {
        "id": "pl0Ug0qrN6Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlweDQZuZBas"
      },
      "outputs": [],
      "source": [
        "index_vec_1[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1fNswIjZMg4"
      },
      "outputs": [],
      "source": [
        "X_train.tail(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HreuFmZ8ZTSF"
      },
      "outputs": [],
      "source": [
        "index_vec_1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLiCjrFyZmjP"
      },
      "outputs": [],
      "source": [
        "index_vec_2.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_vec_2[-1]"
      ],
      "metadata": {
        "id": "99lXlf8MMYWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbXU8p37mL6c"
      },
      "outputs": [],
      "source": [
        "# for shape, we need to give the length of the index-vector for each question\n",
        "# in this case, it should come out to be 10 and the shape will be (10,)\n",
        "# we also could have left the sahpe as (None,) and the model would have inferred it from the acutal data\n",
        "input_1 = Input(shape=(index_vec_1.shape[1],), dtype = 'int64')\n",
        "input_2 = Input(shape=(index_vec_2.shape[1],), dtype = 'int64')\n",
        "\n",
        "lstm_1 = embedding_layer(input_1)\n",
        "lstm_2 = embedding_layer(input_2)\n",
        "\n",
        "\n",
        "common_lstm = LSTM(64,return_sequences=True, activation=\"relu\")\n",
        "vector_1 = common_lstm(lstm_1)\n",
        "vector_1 = Flatten()(vector_1)\n",
        "\n",
        "vector_2 = common_lstm(lstm_2)\n",
        "vector_2 = Flatten()(vector_2)\n",
        "\n",
        "x3 = Subtract()([vector_1, vector_2])\n",
        "x3 = Multiply()([x3, x3])\n",
        "\n",
        "x1_ = Multiply()([vector_1, vector_1])\n",
        "x2_ = Multiply()([vector_2, vector_2])\n",
        "x4 = Subtract()([x1_, x2_])\n",
        "\n",
        "    #https://stackoverflow.com/a/51003359/10650182\n",
        "x5 = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([vector_1, vector_2])\n",
        "\n",
        "conc = Concatenate(axis=-1)([x5,x4, x3])\n",
        "\n",
        "x = Dense(100, activation=\"relu\", name='conc_layer')(conc)\n",
        "x = Dropout(0.01)(x)\n",
        "out = Dense(1, activation=\"sigmoid\", name = 'out')(x)\n",
        "\n",
        "model = Model([input_1, input_2], out)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", metrics=['acc', keras.metrics.AUC(name='auc')], optimizer=Adam(0.00001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP-_1OaE6wKi"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:5]"
      ],
      "metadata": {
        "id": "fYp-M528Migr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get and process validation data\n",
        "# validn_data = pd.read_json(\"sample_data/valid.json\")\n",
        "\n",
        "# test_df_ca = validn.loc[:,['support','correct_answer']].rename(columns={'correct_answer':'response'})\n",
        "# test_df_ca['target'] = 'Similar'\n",
        "# test_df_da1 = test_data.loc[:,['support','distractor1']].rename(columns = {'distractor1':'response'})\n",
        "# test_df_da1['target'] = 'Dissimilar'\n",
        "# test_df_da2 =test_data.loc[:,['support','distractor2']].rename(columns = {'distractor2':'response'})\n",
        "# test_df_da2['target'] = 'Dissimilar'\n",
        "# test_df_da3 =test_data.loc[:,['support','distractor3']].rename(columns = {'distractor3':'response'})\n",
        "# test_df_da3['target'] = 'Dissimilar'\n",
        "\n",
        "# test_data = pd.concat([test_df_ca,test_df_da1,test_df_da2,test_df_da3])\n",
        "\n",
        "valdn_vec_1 = vectorizer([s for s in X_test['support_unstemmed']])\n",
        "valdn_vec_2 = vectorizer([s for s in X_test['response_unstemmed']])\n"
      ],
      "metadata": {
        "id": "8WbGS825MnVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valdn_vec_1[:4]"
      ],
      "metadata": {
        "id": "poXoS5Z3PqqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:2]"
      ],
      "metadata": {
        "id": "zIOyi5OkQKOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.values.reshape(-1,1).shape"
      ],
      "metadata": {
        "id": "Y1BbrZWlRgeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[print(i.shape, i.dtype) for i in model.inputs]\n",
        "[print(o.shape, o.dtype) for o in model.outputs]\n",
        "# [print(l.name, l.input_shape, l.dtype) for l in model.layers]"
      ],
      "metadata": {
        "id": "c8C0oPLBSh1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(index_vec_2)"
      ],
      "metadata": {
        "id": "RBGimb7NS980"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.convert_to_tensor(y_train.values.reshape(-1,1).astype('float32'))"
      ],
      "metadata": {
        "id": "rzUiqMfWBGyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL7lg7LmxxXk"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.fit([index_vec_1,index_vec_2],\n",
        "          tf.convert_to_tensor(y_train.values.reshape(-1,1).astype('float32')),\n",
        "          epochs = 5,\n",
        "          batch_size=64,validation_data=([index_vec_1, index_vec_2],\n",
        "                                         tf.convert_to_tensor(y_train.values.reshape(-1,1).astype('float32')))\n",
        "          )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}